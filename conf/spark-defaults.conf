# Special handling for slow AWS uploads with memory constrained tables
spark.network.timeout   600s
spark.hadoop.fs.s3a.aws.credentials.provider com.amazonaws.auth.WebIdentityTokenCredentialsProvider
spark.jars.packages mysql:mysql-connector-java:8.0.24,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,io.delta:delta-core_2.12:2.1.0,com.johnsnowlabs.nlp:spark-nlp_2.12:4.2.2,org.apache.hadoop:hadoop-aws:3.3.2,org.apache.spark:spark-hadoop-cloud_2.12:3.3.0,com.amazonaws:aws-java-sdk-bundle:1.12.339
spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem
# setting this to false per https://spark.apache.org/docs/latest/sql-performance-tuning.html#coalescing-post-shuffle-partitions
spark.sql.adaptive.coalescePartitions.parallelismFirst false
# https://spot.io/blog/improve-apache-spark-performance-with-the-s3-magic-committer/
# https://github.com/apache/spark/pull/32518#issuecomment-1058840240
# https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html
spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled true
spark.hadoop.fs.s3a.directory.marker.retention keep
spark.hadoop.parquet.enable.summary-metadata false
spark.sql.parquet.mergeSchema false
spark.sql.parquet.filterPushdown true
spark.sql.hive.metastorePartitionPruning true

