:py:mod:`spark_pipeline_framework.transformers.framework_local_file_loader.v1.framework_local_file_loader`
==========================================================================================================

.. py:module:: spark_pipeline_framework.transformers.framework_local_file_loader.v1.framework_local_file_loader


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   spark_pipeline_framework.transformers.framework_local_file_loader.v1.framework_local_file_loader.FrameworkLocalFileLoader




.. py:class:: FrameworkLocalFileLoader(view: str, filepath: Union[str, List[str], pathlib.Path], delimiter: str = ',', limit: int = -1, has_header: bool = True, infer_schema: bool = False, cache_table: bool = True, schema: Optional[pyspark.sql.types.StructType] = None, clean_column_names: bool = False, create_file_path: bool = False, use_schema_from_view: Optional[str] = None, name: Optional[str] = None, parameters: Optional[Dict[str, Any]] = None, progress_logger: Optional[spark_pipeline_framework.progress_logger.progress_logger.ProgressLogger] = None, mode: str = FileReadModes.MODE_PERMISSIVE)

   Bases: :py:obj:`spark_pipeline_framework.transformers.framework_transformer.v1.framework_transformer.FrameworkTransformer`

   Abstract class for transformers that transform one dataset into another.

   .. versionadded:: 1.3.0

   .. py:method:: _transform(self, df: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame

      Transforms the input dataset.

      Parameters
      ----------
      dataset : :py:class:`pyspark.sql.DataFrame`
          input dataset.

      Returns
      -------
      :py:class:`pyspark.sql.DataFrame`
          transformed dataset


   .. py:method:: preprocess(self, df: pyspark.sql.DataFrame, absolute_paths: List[str]) -> None

      Sub-classes can over-ride to do any pre-processing behavior

      :param df: Data Frame
      :param absolute_paths: list of paths to read from


   .. py:method:: getView(self) -> str


   .. py:method:: getFilepath(self) -> Union[str, List[str], pathlib.Path]


   .. py:method:: getSchema(self) -> pyspark.sql.types.StructType


   .. py:method:: getCleanColumnNames(self) -> bool


   .. py:method:: getCacheTable(self) -> bool


   .. py:method:: getInferSchema(self) -> bool


   .. py:method:: getLimit(self) -> int


   .. py:method:: getCreateFilePath(self) -> bool


   .. py:method:: getUseSchemaFromView(self) -> str


   .. py:method:: getName(self) -> str


   .. py:method:: getReaderFormat(self) -> str
      :abstractmethod:


   .. py:method:: getReaderOptions(self) -> Dict[str, Any]
      :abstractmethod:


   .. py:method:: getMode(self) -> str



